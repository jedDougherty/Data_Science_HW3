4a)
After reading in the first 1000 rows and noting the 
difficulties inherent in the raw data,we made several 
decisions about how to handle the data that should be 
noted upfront. First, we used our own way
of reading the data that was more than four times faster
that the lapply, ldply way suggested in the lab. Even so,
the length of time taken increased O(x^2) over time in relation
to the number of rows.As a final problem, my computer - 
a fast new macbook - did not have the memory to hold all 
19 million rows in memory at the same time.


<Insert chart here>


Even with our faster data manipulation algorithm, reading in
the 19,831,300 rows of data and putting them into a data frame
would have taken a prohibitively long time on our pcs. 
To get around that problem I wrote a perl script to randomly
sample the data without having to first process all of it. It
was necessary to randomly sample instead of taking the first 
several thousand rows because the rows were arranged 
chronologically, and we needed to see a sample of the data
across all time, not just in 2008-2010.

This allowed us to take random samples of 19,000 rows (~1/1000th of
the dataset) and 99,000 rows (~1/500th of the dataset).

4b)In our 99,000 person dataset we found that users took
the following actions:
Liked  Disliked  Unwanted Favorited     Saved   Checkin   Comment    Looked      Said 
38348      2335      1366      1456       524     54916         6        17       371 

Extrapolated to the total dataset... we can assume that
users took around the following amounts of actions:
   Liked  Disliked  Unwanted Favorited     Saved   Checkin   Comment    Looked      Said 
7655433    466137    272695    290662    104606  10962912      1198      3394     74063 


------

Determining the number of unique users in the dataset is difficult.
Because a random sampling may not accurately show the number of repeat
users, we took random samplings at multiple sizes and tried to extract
a pattern. There were 31,898 unique users in our data set of 99,340.
If we just took a linear relationship then we would say that the 
entire dataset has 
(31,898) * (total_data_set_size)/(99340) = 6,367,816
But after looking online at getglue articles  we see that they claim only 2 million
unique users, so something is strange. We believe that as we take
larger samples the number of users tapers off -i.e. that User increase
is O(log(x)) to the number of rows in our sample. This concept is discussed further
in the new questions

------

Depending on how you want to define most popular, there are several ways
to answer this question. The first and most simple way we used
was to subtract the number of dislikes from the number of likes for
each movie. We then get the following Movies:

                                        title V1
10:                                 Inception 66
 9:                                   Titanic 67
 8:                               Ratatouille 68
 7:                                Fight Club 68
 6:                              The Hangover 69
 5:                              Forrest Gump 70
 4:                                    Avatar 78
 3:                                        Up 79
 2: Lord of the Rings: The Return of the King 81
 1:                                  Iron Man 82
What a bunch of nerds on this site!
In comparison here are the ten least liked movies:
                                    title V1
 1:                               Evening -2
 2: The Fast and the Furious: Tokyo Drift -2
 3:                             Max Payne -2
 4:                Left Behind: The Movie -2
 5:                   Tomorrow Never Dies -2
 6:                    The Pink Panther 2 -2
 7:                                  2046 -2
 8:                         The Good Girl -2
 9:                       Dressed to Kill -2
10:                               4.3.2.1 -2
Yep! Those films pretty much suck. Except for
The Fast and the Furious: Tokyo Drift. We're convinced
that bad boy is going to become a cult classic.

Raw popularity can also be measured by number of check ins.
The ten films with the most check ins are as follows:
                                            title  V1
10:                                 Puss In Boots  78
 9:                                    The Smurfs  79
 8:                          Dr. Seuss' The Lorax  85
 7:                                  Dark Shadows  86
 6:            Captain America: The First Avenger  97
 5: Harry Potter and the Deathly Hallows: Part II 103
 4:                                   The Muppets 120
 3:                         Marvel's The Avengers 294
 2:                              The Hunger Games 340
 1:       The Twilight Saga: Breaking Dawn Part 1 388

Wow that is a totally different list. A theory for this
is that people's favorite movies often came out before
the site started, meaning that only rewatches would show
up as check ins.

------

Instead of using R, I just opened up the whole dataset
in Vim and used the following line to count the numberof timestamps:
of timestamps::
%s/\"timestamp\": \"2011-\d\d-\d\d//gn
This returned 10709526 actions in 2011
We can do the same thing with our sample set in R.
year_articles <- length(grep("^2011",listings_onek$timestamp,value=TRUE))
By multiplying by (total_data_set_size)/(99340)
we get: 10631725, which is very similar to the actual number

------


