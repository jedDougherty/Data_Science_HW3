4a)
After reading in the first 1000 rows and noting the 
difficulties inherent in the raw data,we made several 
decisions about how to handle the data that should be 
noted upfront. First, we used our own way
of reading the data that was more than four times faster
that the lapply, ldply way suggested in the lab. Even so,
the length of time taken increased O(x^2) over time in relation
to the number of rows.As a final problem, my computer - 
a fast new macbook - did not have the memory to hold all 
19 million rows in memory at the same time.


<Insert chart here>


Even with our faster data manipulation algorithm, reading in
the 19,831,300 rows of data and putting them into a data frame
would have taken a prohibitively long time on our pcs. 
To get around that problem I wrote a perl script to randomly
sample the data without having to first process all of it. It
was necessary to randomly sample instead of taking the first 
several thousand rows because the rows were arranged 
chronologically, and we needed to see a sample of the data
across all time, not just in 2008-2010.

This allowed us to take random samples of 19,000 rows (~1/1000th of
the dataset) and 98,000 rows (~1/500th of the dataset).

4b)In our 98,000 person dataset we found that users took
the following actions:
Liked  Disliked  Unwanted Favorited     Saved   Checkin   Comment    Looked      Said 
38348      2335      1366      1456       524     54916         6        17       371 

Extrapolated to the total dataset... we can assume that
users took around the following amounts of actions:
   Liked  Disliked  Unwanted Favorited     Saved   Checkin   Comment    Looked      Said 
7655433    466137    272695    290662    104606  10962912      1198      3394     74063 

Determining the number of unique users in the dataset is difficult.
Because a random sampling may not accurately show the number of repeat
users, we took random samplings at multiple sizes and tried to extract
a pattern.
